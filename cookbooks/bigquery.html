<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Accessing and working with BigQuery - Firefox Data Documentation</title>
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="../dtmo.css">
        
        <link rel="stylesheet" href="../mermaid.css">
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox" class="sidebar-scrollbox">
                <ol class="chapter"><li class="expanded affix "><a href="../introduction.html">Firefox Data Documentation</a></li><li class="expanded "><a href="../concepts/reporting_a_problem.html"><strong aria-hidden="true">1.</strong> Reporting a problem</a></li><li class="expanded "><a href="../concepts/terminology.html"><strong aria-hidden="true">2.</strong> Terminology</a></li><li class="expanded "><a href="../concepts/getting_started.html"><strong aria-hidden="true">3.</strong> Getting Started</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/analysis_intro.html"><strong aria-hidden="true">3.1.</strong> Analysis Quick Start</a></li><li class="expanded "><a href="../concepts/choosing_a_dataset.html"><strong aria-hidden="true">3.2.</strong> Choosing a Desktop Dataset</a></li><li class="expanded "><a href="../concepts/choosing_a_dataset_mobile.html"><strong aria-hidden="true">3.3.</strong> Choosing a Mobile Dataset</a></li><li class="expanded "><a href="../tools/stmo.html"><strong aria-hidden="true">3.4.</strong> Intro to STMO</a></li><li class="expanded "><a href="../concepts/analysis_gotchas.html"><strong aria-hidden="true">3.5.</strong> Common Analysis Gotchas</a></li><li class="expanded "><a href="../concepts/sql_optimization.html"><strong aria-hidden="true">3.6.</strong> Optimizing Queries</a></li><li class="expanded "><a href="../concepts/getting_help.html"><strong aria-hidden="true">3.7.</strong> Getting Help</a></li></ol></li><li class="expanded "><a href="../tools/index.html"><strong aria-hidden="true">4.</strong> Tools</a></li><li><ol class="section"><li class="expanded "><a href="../tools/projects.html"><strong aria-hidden="true">4.1.</strong> Project Glossary</a></li><li class="expanded "><a href="../concepts/pipeline/gcp_data_pipeline.html"><strong aria-hidden="true">4.2.</strong> Overview of Mozilla's Data Pipeline</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/pipeline/http_edge_spec.html"><strong aria-hidden="true">4.2.1.</strong> HTTP Edge Server Specification</a></li><li class="expanded "><a href="../concepts/pipeline/event_pipeline.html"><strong aria-hidden="true">4.2.2.</strong> Event Pipeline Detail</a></li><li class="expanded "><a href="../concepts/pipeline/data_pipeline.html"><strong aria-hidden="true">4.2.3.</strong> Previous AWS Pipeline Overview</a></li><li class="expanded "><a href="../concepts/pipeline/data_pipeline_detail.html"><strong aria-hidden="true">4.2.4.</strong> In-depth AWS Data Pipeline Detail</a></li></ol></li><li class="expanded "><a href="../tools/interfaces.html"><strong aria-hidden="true">4.3.</strong> Analysis Interfaces</a></li><li class="expanded "><a href="../tools/spark.html"><strong aria-hidden="true">4.4.</strong> Custom analysis with Spark</a></li><li class="expanded "><a href="../concepts/sql_style.html"><strong aria-hidden="true">4.5.</strong> SQL Style Guide</a></li><li class="expanded "><a href="../concepts/glean/glean.html"><strong aria-hidden="true">4.6.</strong> Glean overview</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/glean/debug_ping_view.html"><strong aria-hidden="true">4.6.1.</strong> Glean Debug ping viewer</a></li></ol></li><li class="expanded "><a href="../tools/alerts.html"><strong aria-hidden="true">4.7.</strong> Alerts</a></li></ol></li><li class="expanded "><a href="../cookbooks/index.html"><strong aria-hidden="true">5.</strong> Analysis cookbooks</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/bigquery.html" class="active"><strong aria-hidden="true">5.1.</strong> Accessing and working with BigQuery</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/bigquery-airflow.html"><strong aria-hidden="true">5.1.1.</strong> Scheduling BigQuery Queries in Airflow</a></li></ol></li><li class="expanded "><a href="../cookbooks/dataset_specific.html"><strong aria-hidden="true">5.2.</strong> Dataset Specific</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/normandy_events.html"><strong aria-hidden="true">5.2.1.</strong> Working with Normandy events</a></li></ol></li><li class="expanded "><a href="../cookbooks/realtime.html"><strong aria-hidden="true">5.3.</strong> Real-time</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/view_pings_cep.html"><strong aria-hidden="true">5.3.1.</strong> Seeing Your Own Pings</a></li></ol></li><li class="expanded "><a href="../cookbooks/metrics.html"><strong aria-hidden="true">5.4.</strong> Metrics</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/dau.html"><strong aria-hidden="true">5.4.1.</strong> Daily Active Users (DAU)</a></li><li class="expanded "><a href="../cookbooks/active_dau.html"><strong aria-hidden="true">5.4.2.</strong> Active DAU (aDAU)</a></li><li class="expanded "><a href="../cookbooks/retention.html"><strong aria-hidden="true">5.4.3.</strong> Retention</a></li></ol></li></ol></li><li class="expanded "><a href="../datasets/new_data.html"><strong aria-hidden="true">6.</strong> Sending telemetry</a></li><li><ol class="section"><li class="expanded "><a href="../cookbooks/client_guidelines.html"><strong aria-hidden="true">6.1.</strong> Implementing Experiments</a></li><li class="expanded "><a href="../cookbooks/events_best_practices.html"><strong aria-hidden="true">6.2.</strong> Sending Events</a></li><li class="expanded "><a href="../cookbooks/new_ping.html"><strong aria-hidden="true">6.3.</strong> Sending a Custom Ping</a></li><li class="spacer"></li></ol></li><li class="expanded "><a href="../datasets/reference.html"><strong aria-hidden="true">7.</strong> Dataset Reference</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/pings.html"><strong aria-hidden="true">7.1.</strong> Pings</a></li><li class="expanded "><a href="../datasets/derived.html"><strong aria-hidden="true">7.2.</strong> Derived Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/active_profiles.html"><strong aria-hidden="true">7.2.1.</strong> Active Profiles</a></li><li class="expanded "><a href="../datasets/batch_view/addons/reference.html"><strong aria-hidden="true">7.2.2.</strong> Addons</a></li><li class="expanded "><a href="../datasets/other/addons_daily/reference.html"><strong aria-hidden="true">7.2.3.</strong> Addons Daily</a></li><li class="expanded "><a href="../datasets/batch_view/clients_daily/reference.html"><strong aria-hidden="true">7.2.4.</strong> Clients Daily</a></li><li class="expanded "><a href="../datasets/bigquery/clients_last_seen/reference.html"><strong aria-hidden="true">7.2.5.</strong> Clients Last Seen</a></li><li class="expanded "><a href="../datasets/batch_view/events/reference.html"><strong aria-hidden="true">7.2.6.</strong> Events</a></li><li class="expanded "><a href="../datasets/bigquery/exact_mau/reference.html"><strong aria-hidden="true">7.2.7.</strong> Exact MAU</a></li><li class="expanded "><a href="../datasets/batch_view/first_shutdown_summary/reference.html"><strong aria-hidden="true">7.2.8.</strong> First Shutdown Summary</a></li><li class="expanded "><a href="../datasets/batch_view/main_summary/reference.html"><strong aria-hidden="true">7.2.9.</strong> Main Summary</a></li><li class="expanded "><a href="../datasets/batch_view/new_profile/reference.html"><strong aria-hidden="true">7.2.10.</strong> New Profile</a></li><li class="expanded "><a href="../datasets/other/socorro_crash/reference.html"><strong aria-hidden="true">7.2.11.</strong> Socorro Crash Reports</a></li><li class="expanded "><a href="../datasets/other/ssl/reference.html"><strong aria-hidden="true">7.2.12.</strong> SSL Ratios (public)</a></li><li class="expanded "><a href="../datasets/batch_view/telemetry_aggregates/reference.html"><strong aria-hidden="true">7.2.13.</strong> Telemetry Aggregates</a></li><li class="expanded "><a href="../datasets/batch_view/update/reference.html"><strong aria-hidden="true">7.2.14.</strong> Update</a></li></ol></li><li class="expanded "><a href="../tools/experiments.html"><strong aria-hidden="true">7.3.</strong> Experimental Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/heartbeat.html"><strong aria-hidden="true">7.3.1.</strong> Accessing Heartbeat data</a></li><li class="expanded "><a href="../datasets/shield.html"><strong aria-hidden="true">7.3.2.</strong> Accessing Shield Study data</a></li></ol></li><li class="expanded "><a href="../datasets/search.html"><strong aria-hidden="true">7.4.</strong> Search Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/search/search_aggregates/reference.html"><strong aria-hidden="true">7.4.1.</strong> Search Aggregates</a></li><li class="expanded "><a href="../datasets/search/search_clients_daily/reference.html"><strong aria-hidden="true">7.4.2.</strong> Search Clients Daily</a></li></ol></li><li class="expanded "><a href="../datasets/other.html"><strong aria-hidden="true">7.5.</strong> Other Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/other/hgpush/reference.html"><strong aria-hidden="true">7.5.1.</strong> hgpush</a></li><li class="expanded "><a href="../datasets/other/stub_installer/reference.html"><strong aria-hidden="true">7.5.2.</strong> Stub installer ping</a></li><li class="expanded "><a href="../datasets/other/activity-stream/reference.html"><strong aria-hidden="true">7.5.3.</strong> Activity Stream</a></li></ol></li><li class="expanded "><a href="../datasets/obsolete.html"><strong aria-hidden="true">7.6.</strong> Obsolete Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/obsolete/churn/reference.html"><strong aria-hidden="true">7.6.1.</strong> Churn</a></li><li class="expanded "><a href="../datasets/obsolete/client_count/reference.html"><strong aria-hidden="true">7.6.2.</strong> Client Count</a></li><li class="expanded "><a href="../datasets/obsolete/client_count_daily/reference.html"><strong aria-hidden="true">7.6.3.</strong> Client Count Daily</a></li><li class="expanded "><a href="../datasets/obsolete/crash_aggregates/reference.html"><strong aria-hidden="true">7.6.4.</strong> Crash Aggregates</a></li><li class="expanded "><a href="../datasets/obsolete/crash_summary/reference.html"><strong aria-hidden="true">7.6.5.</strong> Crash Summary</a></li><li class="expanded "><a href="../datasets/obsolete/error_aggregates/reference.html"><strong aria-hidden="true">7.6.6.</strong> Error Aggregates</a></li><li class="expanded "><a href="../datasets/obsolete/heavy_users/reference.html"><strong aria-hidden="true">7.6.7.</strong> Heavy Users</a></li><li class="expanded "><a href="../datasets/obsolete/longitudinal/reference.html"><strong aria-hidden="true">7.6.8.</strong> Longitudinal</a></li><li class="expanded "><a href="../datasets/obsolete/retention/reference.html"><strong aria-hidden="true">7.6.9.</strong> Retention</a></li><li class="expanded "><a href="../datasets/obsolete/sync_summary/reference.html"><strong aria-hidden="true">7.6.10.</strong> Sync Summary</a></li></ol></li><li class="expanded "><a href="../datasets/fxa.html"><strong aria-hidden="true">7.7.</strong> Firefox Accounts Datasets</a></li><li><ol class="section"><li class="expanded "><a href="../datasets/fxa_metrics/attribution.html"><strong aria-hidden="true">7.7.1.</strong> Firefox Account Attribution</a></li><li class="expanded "><a href="../datasets/fxa_metrics/funnels.html"><strong aria-hidden="true">7.7.2.</strong> Firefox Account Funnel Metrics</a></li><li class="expanded "><a href="../datasets/fxa_metrics/emails.html"><strong aria-hidden="true">7.7.3.</strong> Firefox Account Email Metrics</a></li><li class="spacer"></li></ol></li></ol></li><li class="expanded "><a href="../concepts/index.html"><strong aria-hidden="true">8.</strong> Telemetry Behavior Reference</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/history.html"><strong aria-hidden="true">8.1.</strong> History of Telemetry</a></li><li class="expanded "><a href="../concepts/profile/index.html"><strong aria-hidden="true">8.2.</strong> Profile Behavior</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/profile/profile_creation.html"><strong aria-hidden="true">8.2.1.</strong> Profile Creation</a></li><li class="expanded "><a href="../concepts/profile/realworldusage.html"><strong aria-hidden="true">8.2.2.</strong> Real World Usage</a></li><li class="expanded "><a href="../concepts/profile/profilehistory.html"><strong aria-hidden="true">8.2.3.</strong> Profile History</a></li></ol></li><li class="expanded "><a href="../concepts/channels/index.html"><strong aria-hidden="true">8.3.</strong> Channel Behavior</a></li><li><ol class="section"><li class="expanded "><a href="../concepts/channels/channel_normalization.html"><strong aria-hidden="true">8.3.1.</strong> Channel Normalization and Querying</a></li></ol></li><li class="expanded "><a href="../concepts/censuses.html"><strong aria-hidden="true">8.4.</strong> Census metrics</a></li><li class="expanded "><a href="../concepts/engagement.html"><strong aria-hidden="true">8.5.</strong> Engagement metrics</a></li><li class="expanded "><a href="../concepts/sample_id.html"><strong aria-hidden="true">8.6.</strong> Sampling</a></li><li class="spacer"></li></ol></li><li class="expanded "><a href="../meta/index.html"><strong aria-hidden="true">9.</strong> About this Documentation</a></li><li><ol class="section"><li class="expanded "><a href="../meta/contributing.html"><strong aria-hidden="true">9.1.</strong> Contributing</a></li><li class="expanded "><a href="../meta/structure.html"><strong aria-hidden="true">9.2.</strong> Structure</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">Firefox Data Documentation</h1>

                        <div class="right-buttons">
                            <a href="../print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#accessing-and-working-with-bigquery" id="accessing-and-working-with-bigquery">Accessing and working with BigQuery</a></h1>
<p>This guide will give you a quick introduction to working with data stored
in <a href="https://cloud.google.com/bigquery/">BigQuery</a></p>
<p>BigQuery uses a columnar data storage format called <a href="https://cloud.google.com/blog/products/gcp/inside-capacitor-bigquerys-next-generation-columnar-storage-format">Capacitor</a> which supports semi-structured data.</p>
<p>There is a cost associated with using BigQuery based on operations. As of right now we pay an on-demand pricing for queries based on how much data a query scans. To minimize costs see <a href="bigquery.html#query-optimizations"><em>Query Optimizations</em></a>. More detailed pricing information can be found <a href="https://cloud.google.com/bigquery/pricing">here</a>.</p>
<p>With the transition to <a href="https://cloud.google.com">GCP</a> in 2019, BigQuery has become our primary data warehouse and
SQL Query engine.
Our previous SQL Query Engines, Presto and Athena, and our Parquet data lake will no longer be accessible
by the end of 2019.
Specific guidance for transitioning off of the AWS data
infrastructure, including up-to-date timelines of data availability, is
maintained in the <a href="https://docs.google.com/document/d/1nlzhRGGwAaClwbotd0oWnnkB5GcvpodIxN3Dk5vWvNI/edit#">Data Access Continuity Guide</a> Google Doc.</p>
<h2><a class="header" href="#table-of-contents" id="table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#access">Access</a>
<ul>
<li><a href="#interfaces">Interfaces</a></li>
<li><a href="#access-request">Access Request</a></li>
<li><a href="#from-redash">From re:dash</a></li>
<li><a href="#gcp-bigquery-console">GCP BigQuery Console</a></li>
<li><a href="#gcp-bigquery-api-access">GCP BigQuery API Access</a>
<ul>
<li><a href="#from-">From </a></li>
<li><a href="#bq"><code>bq</code></a></li>
<li><a href="#-command-line-tool"> Command-line Tool</a>
<ul>
<li><a href="#bq"><code>bq</code></a></li>
<li><a href="#-examples"> Examples</a></li>
</ul>
</li>
<li><a href="#from-client-sdks">From client SDKs</a></li>
</ul>
</li>
<li><a href="#from-spark">From Spark</a>
<ul>
<li><a href="#on-databricks">On Databricks</a></li>
<li><a href="#on-dataproc">On Dataproc</a></li>
</ul>
</li>
<li><a href="#from-colaboratory">From Colaboratory</a></li>
</ul>
</li>
<li><a href="#querying-tables">Querying Tables</a>
<ul>
<li><a href="#projects-datasets-and-tables-in-bigquery">Projects, Datasets and Tables in BigQuery</a>
<ul>
<li><a href="#caveats">Caveats</a></li>
<li><a href="#projects-with-bigquery-datasets">Projects with BigQuery datasets</a></li>
<li><a href="#table-layout-and-naming">Table Layout and Naming</a></li>
<li><a href="#structure-of-ping-tables-in-bigquery">Structure of Ping Tables in BigQuery</a></li>
<li><a href="#writing-queries">Writing Queries</a></li>
<li><a href="#writing-query-results-to-a-permanent-table">Writing query results to a permanent table</a></li>
<li><a href="#writing-results-to-gcs-object-store">Writing results to GCS (object store)</a></li>
<li><a href="#creating-a-view">Creating a View</a></li>
<li><a href="#using-udfs">Using UDFs</a>
<ul>
<li><a href="#accessing-map-like-fields">Accessing map-like fields</a></li>
<li><a href="#accessing-histograms">Accessing histograms</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#query-optimizations">Query Optimizations</a></li>
</ul>
<h1><a class="header" href="#access" id="access">Access</a></h1>
<p>There are multiple ways to access BigQuery. For most users the primary interface will be <a href="https://sql.telemetry.mozilla.org/">re:dash</a>.</p>
<p>See below for additional interfaces. All other interfaces will require access to be provisioned.</p>
<h2><a class="header" href="#interfaces" id="interfaces">Interfaces</a></h2>
<p>BigQuery datasets and tables can be accessed by the following methods:</p>
<ul>
<li><a href="bigquery.html#from-redash">re:dash</a></li>
<li><a href="bigquery.html#gcp-bigquery-console">GCP BigQuery Console</a>
<ul>
<li>For advanced use cases including managing query outputs, table management. Requires GCP access to be granted by Data Operations.</li>
</ul>
</li>
<li><a href="bigquery.html#gcp-bigquery-api-access">GCP BigQuery API Access</a>
<ul>
<li>For advanced use cases including automated workloads, ETL, <a href="https://cloud.google.com/bigquery/docs/reference/storage/">BigQuery Storage API</a>. Requires GCP access to be granted by Data Operations.</li>
<li>Allows access to BigQuery via <a href="https://cloud.google.com/bigquery/docs/bq-command-line-tool"><code>bq</code> command-line tool</a></li>
</ul>
</li>
<li><a href="bigquery.html#from-spark">Spark</a>
<ul>
<li><a href="bigquery.html#on-databricks">Databricks</a></li>
<li><a href="bigquery.html#on-dataproc">Dataproc</a></li>
</ul>
</li>
<li><a href="bigquery.html#from-colaboratory">Colaboratory</a></li>
</ul>
<h2><a class="header" href="#access-request" id="access-request">Access Request</a></h2>
<p>For access to BigQuery via GCP Console and API please file a bug <a href="https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=jthomas%40mozilla.com&amp;bug_file_loc=https%3A%2F%2Fmana.mozilla.org%2Fwiki%2Fx%2FiIPeB&amp;bug_ignored=0&amp;bug_severity=normal&amp;bug_status=NEW&amp;bug_type=task&amp;cf_fx_iteration=---&amp;cf_fx_points=---&amp;comment=Please%20grant%20me%20access%20to%20the%20BigQuery%20GCP%20console%20and%20API%20Access.%20I%20work%20on%20%3Cteam%3E.%0D%0A%0D%0AMy%20mozilla.com%20ldap%20login%20is%20%3Cyour%20ldap%20login%3E%40mozilla.com.&amp;component=Operations&amp;contenttypemethod=list&amp;contenttypeselection=text%2Fplain&amp;defined_groups=1&amp;flag_type-4=X&amp;flag_type-607=X&amp;flag_type-800=X&amp;flag_type-803=X&amp;flag_type-936=X&amp;form_name=enter_bug&amp;maketemplate=Remember%20values%20as%20bookmarkable%20template&amp;op_sys=Unspecified&amp;priority=--&amp;product=Data%20Platform%20and%20Tools&amp;qa_contact=jthomas%40mozilla.com&amp;rep_platform=Unspecified&amp;short_desc=BigQuery%20GCP%20Console%20and%20API%20Access%20for%20%3Cyour%20ldap%20login%3E%40mozilla.com&amp;target_milestone=---&amp;version=unspecified">here</a>. As part of this request we will add you to the appropriate Google Groups and provision a GCP Service Account.</p>
<h2><a class="header" href="#from-redash" id="from-redash">From re:dash</a></h2>
<p>All Mozilla users will be able to access BigQuery via <a href="https://sql.telemetry.mozilla.org/">re:dash</a> through the following Data Sources:</p>
<ul>
<li><code>Telemetry (BigQuery)</code></li>
<li><code>Telemetry Search (BigQuery)</code>
<ul>
<li>This group is restricted to users in the re:dash <code>search</code> group.</li>
</ul>
</li>
</ul>
<p>Access via re:dash is read-only. You will not be able to create views or tables via re:dash.</p>
<h2><a class="header" href="#gcp-bigquery-console" id="gcp-bigquery-console">GCP BigQuery Console</a></h2>
<ul>
<li>File a <a href="bigquery.html#access-request">bug</a> with Data Operations for access to GCP Console.</li>
<li>Visit <a href="https://console.cloud.google.com/bigquery">GCP BigQuery Console</a></li>
<li>Switch to the project provided to you during your access request e.g <code>moz-fx-data-bq-&lt;team-name&gt;</code></li>
</ul>
<p>See <a href="https://cloud.google.com/bigquery/docs/bigquery-web-ui">Using the BigQuery web UI in the GCP Console</a> for more details.</p>
<h2><a class="header" href="#gcp-bigquery-api-access" id="gcp-bigquery-api-access">GCP BigQuery API Access</a></h2>
<ul>
<li>File a <a href="bigquery.html#access-request">bug</a> with Data Operations for access to GCP BigQuery API Access.</li>
</ul>
<p>A list of supported BigQuery client libraries can be found <a href="https://cloud.google.com/bigquery/docs/reference/libraries">here</a>.</p>
<p>Detailed REST reference can be found <a href="https://cloud.google.com/bigquery/docs/reference/rest/">here</a>.</p>
<h3><a class="header" href="#from-bq-command-line-tool" id="from-bq-command-line-tool">From <code>bq</code> Command-line Tool</a></h3>
<ul>
<li>Install the <a href="https://cloud.google.com/sdk/">GCP SDK</a></li>
<li>Authorize <code>gcloud</code> with either your user account or provisioned service account. See documentation <a href="https://cloud.google.com/sdk/docs/authorizing">here</a>.
<ul>
<li><code>gcloud auth login</code></li>
</ul>
</li>
<li>Set your google project to your team project
<ul>
<li><code>gcloud config set project moz-fx-data-bq-&lt;team-name&gt;</code></li>
<li>project name will be provided for you when your account is provisioned.</li>
</ul>
</li>
</ul>
<h4><a class="header" href="#bq-examples" id="bq-examples"><code>bq</code> Examples</a></h4>
<p>List tables and views in a BigQuery dataset</p>
<pre><code class="language-bash">bq ls moz-fx-data-derived-datasets:telemetry
</code></pre>
<p>Query a table or view</p>
<pre><code class="language-bash">bq query --nouse_legacy_sql 'select count(*) from `moz-fx-data-derived-datasets.telemetry.main` where submission_date = &quot;2019-08-22&quot; LIMIT 10'
</code></pre>
<p>Additional examples and documentation can be found <a href="https://cloud.google.com/bigquery/docs/bq-command-line-tool">here</a>.</p>
<h3><a class="header" href="#from-client-sdks" id="from-client-sdks">From client SDKs</a></h3>
<p>Client SDKs for various programming languages don't access credentials the
same way as the <code>gcloud</code> and <code>bq</code> command-line tools. The client SDKs
generally assume that the machine is configured with a service account and
will look for JSON-based credentials in several well-known locations rather
than looking for user credentials.</p>
<p>If you have service account credentials, you can point client SDKs at them
by setting:</p>
<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/creds.json
</code></pre>
<p>If you don't have appropriate service account credentials, but your GCP user
account has sufficient access, you can have your user credentials mimic a
service account by running:</p>
<pre><code>gcloud auth application-default login
</code></pre>
<p>Once you've followed the browser flow to grant access, you should be able to,
for example, access BigQuery from Python:</p>
<pre><code>pip install google-cloud-bigquery
python -c 'from google.cloud import bigquery; print([d.dataset_id for d in bigquery.Client().list_datasets()])'
</code></pre>
<h2><a class="header" href="#from-spark" id="from-spark">From Spark</a></h2>
<p>We recommend the <a href="https://github.com/GoogleCloudPlatform/spark-bigquery-connector">Storage API Connector</a> for accessing
BigQuery tables in Spark as it is the most modern and actively developed connector. It works well with the BigQuery
client library which is useful if you need to run arbitrary SQL queries (see example Databricks notebook) and load their
results into Spark.</p>
<h3><a class="header" href="#on-databricks" id="on-databricks">On Databricks</a></h3>
<p>The <code>shared_serverless_python3</code> cluster is configured with shared default GCP credentials that will be automatically picked
up by BigQuery client libraries. It also has the Storage API Connector library added as seen in the example
<a href="https://dbc-caf9527b-e073.cloud.databricks.com/#notebook/141939">Python notebook</a>.</p>
<h3><a class="header" href="#on-dataproc" id="on-dataproc">On Dataproc</a></h3>
<p>Dataproc is Google's managed Spark cluster service. Accessing BigQuery from there will be faster than from Databricks
because it will not involve cross-cloud data transfers.</p>
<p>You can spin up a Dataproc cluster with Jupyter using the following command. Insert your values for <code>cluster-name</code>, <code>bucket-name</code>, and <code>project-id</code> there. Your notebooks will be stored in Cloud Storage under <code>gs://bucket-name/notebooks/jupyter</code>:</p>
<pre><code class="language-bash">gcloud beta dataproc clusters create cluster-name \
    --optional-components=ANACONDA,JUPYTER \
    --image-version=1.4 \
    --enable-component-gateway \
    --properties=^#^spark:spark.jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar \
    --num-workers=3 \
    --max-idle=3h \
    --bucket bucket-name \
    --region=us-west1 \
    --project project-id
</code></pre>
<p>Jupyter URL can be retrieved with the following command:</p>
<pre><code class="language-bash">gcloud beta dataproc clusters describe cluster-name --region=us-west1 --project project-id | grep Jupyter
</code></pre>
<p>After you've finished your work, it's a good practice to delete your cluster:</p>
<pre><code class="language-bash">gcloud beta dataproc clusters delete cluster-name --region=us-west1 --project project-id --quiet
</code></pre>
<h2><a class="header" href="#from-colaboratory" id="from-colaboratory">From Colaboratory</a></h2>
<p><a href="https://colab.research.google.com">Colaboratory</a> is Jupyter notebook environment, managed by Google and running in the cloud. Notebooks are stored in Google Drive and can be shared in a similar way to Google Docs.</p>
<p>Colaboratory can be used to easily access BigQuery and perform interactive analyses. See <a href="https://colab.research.google.com/drive/1uXmrPnqzDATiCVH2RNJKD8obIZuofFHx"><code>Telemetry Hello World</code> notebook</a>.</p>
<p>Note: this is very similar to <a href="bigquery.html#gcp-bigquery-api-access">API Access</a>, so you will need access to your team's GCP project - file a request as described <a href="bigquery.html#access-request">above</a>.</p>
<h1><a class="header" href="#querying-tables" id="querying-tables">Querying Tables</a></h1>
<h2><a class="header" href="#projects-datasets-and-tables-in-bigquery" id="projects-datasets-and-tables-in-bigquery">Projects, Datasets and Tables in BigQuery</a></h2>
<p>In GCP a <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects">project</a> is a way to organize cloud resources. We use multiple
projects to maintain our BigQuery <a href="https://cloud.google.com/bigquery/docs/datasets-intro">datasets</a>.</p>
<p>Note that we have historically used the term <em>dataset</em> to describe a set of
records all following the same schema, but this idea corresponds to a <em>table</em>
in BigQuery. In BigQuery terminology,
datasets are a top-level container used to organize and
control access to tables and views.</p>
<h3><a class="header" href="#caveats" id="caveats">Caveats</a></h3>
<ul>
<li>The date partition field (e.g. <code>submission_date_s3</code>, <code>submission_date</code>) is mostly used as a partitioning column,
but it has changed from <code>YYYYMMDD</code> string form to a proper <code>DATE</code> type that accepts string literals in the more standards-friendly <code>YYYY-MM-DD</code> form.</li>
<li>Unqualified queries can become very costly very easily. We've placed restrictions on large tables from accidentally querying &quot;all data for all time&quot;,
namely that you must make use of the date partition fields for large tables (like <code>main_summary</code> or <code>clients_daily</code>).</li>
<li>Please read <a href="bigquery.html#query-optimizations"><em>Query Optimizations</em></a> section that contains advice on how to reduce cost and improve query performance.</li>
<li>re:dash BigQuery data sources will have a 10 TB data scanned limit per query. Please let us know in <code>#fx-metrics</code> on Slack if you run into issues!</li>
<li>There is no native map support in BigQuery. Instead, we are using structs with fields [key, value]. We have provided convenience functions to access these like key-value maps (described <a href="bigquery.html#accessing-map-like-fields">below</a>.)</li>
</ul>
<h3><a class="header" href="#projects-with-bigquery-datasets" id="projects-with-bigquery-datasets">Projects with BigQuery datasets</a></h3>
<table><thead><tr><th>Project</th><th>Dataset</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>moz-fx-data-shared-prod</code></td><td></td><td>All production data including full pings, imported parquet data, <a href="https://github.com/mozilla/bigquery-etl">BigQuery ETL</a>, and ad-hoc analysis</td></tr>
<tr><td></td><td><code>&lt;namespace&gt;_live</code></td><td>See <em>live datasets</em> below</td></tr>
<tr><td></td><td><code>&lt;namespace&gt;_stable</code></td><td>See <em>stable datasets</em> below</td></tr>
<tr><td></td><td><code>&lt;namespace&gt;_derived</code></td><td>See <em>derived datasets</em> below</td></tr>
<tr><td></td><td><code>&lt;namespace&gt;</code></td><td>See <em>user-facing (unsuffixed) datasets</em> below</td></tr>
<tr><td></td><td><code>analysis</code></td><td>User generated tables for analysis</td></tr>
<tr><td></td><td><code>backfill</code></td><td>Temporary staging area for back-fills</td></tr>
<tr><td></td><td><code>blpadi</code></td><td>Blocklist ping derived data(<em>restricted</em>)</td></tr>
<tr><td></td><td><code>payload_bytes_raw</code></td><td>Raw JSON payloads as received from clients, used for reprocessing scenarios, a.k.a. &quot;landfill&quot; (<em>restricted</em>)</td></tr>
<tr><td></td><td><code>payload_bytes_decoded</code></td><td><code>gzip</code>-compressed decoded JSON payloads, used for reprocessing scenarios</td></tr>
<tr><td></td><td><code>payload_bytes_error</code></td><td><code>gzip</code>-compressed JSON payloads that were rejected in some phase of the pipeline; particularly useful for investigating schema validation errors</td></tr>
<tr><td></td><td><code>search</code></td><td>Search data imported from parquet (<em>restricted</em>)</td></tr>
<tr><td></td><td><code>static</code></td><td>Static tables, often useful for data-enriching joins</td></tr>
<tr><td></td><td><code>tmp</code></td><td>Temporary staging area for parquet data loads</td></tr>
<tr><td></td><td><code>udf</code></td><td>Persistent user-defined functions defined in SQL; see <a href="#using-udfs">Using UDFs</a></td></tr>
<tr><td></td><td><code>udf_js</code></td><td>Persistent user-defined functions defined in JavaScript; see <a href="#using-udfs">Using UDFs</a></td></tr>
<tr><td></td><td><code>validation</code></td><td>Temporary staging area for validation</td></tr>
<tr><td><code>moz-fx-data-derived-datasets</code></td><td></td><td>Legacy project that contains mostly views to data in <code>moz-fx-data-shared-prod</code> during a transition period; STMO currently points at this project but we will announce a transition to <code>moz-fx-data-shared-prod</code> by end of 2019</td></tr>
<tr><td></td><td><code>analysis</code></td><td>User generated tables for analysis; note that this dataset is separate from <code>moz-fx-data-shared-prod:analysis</code> and users are responsible for migrating or cloning data during the transition period</td></tr>
<tr><td><code>moz-fx-data-shar-nonprod-efed</code></td><td></td><td>Non-production data produced by stage ingestion infrastructure</td></tr>
</tbody></table>
<h3><a class="header" href="#table-layout-and-naming" id="table-layout-and-naming">Table Layout and Naming</a></h3>
<p>Under the single <code>moz-fx-data-shared-prod</code> project,
each document namespace (corresponding to folders underneath the <a href="https://github.com/mozilla-services/mozilla-pipeline-schemas/tree/master/schemas">schemas directory of <code>mozilla-pipeline-schemas</code></a>) has four BigQuery datasets provisioned with the following properties:</p>
<ul>
<li><em>Live datasets</em> (<code>telemetry_live</code>, <code>activity_stream_live</code>, etc.) contain live ping tables (see definitions of table types in the next paragraph)</li>
<li><em>Stable datasets</em> (<code>telemetry_stable</code>, <code>activity_stream_stable</code>, etc.) contain historical ping tables</li>
<li><em>Derived datasets</em> (<code>telemetry_derived</code>, <code>activity_stream_derived</code>, etc.) contain derived tables, primarily populated via nightly queries defined in <a href="https://github.com/mozilla/bigquery-etl">BigQuery ETL</a> and managed by Airflow</li>
<li><em>User-facing (unsuffixed) datasets</em> (<code>telemetry</code>, <code>activity_stream</code>, etc.) contain user-facing views on top of the tables in the corresponding stable and derived datasets.</li>
</ul>
<p>The table and view types referenced above are defined as follows:</p>
<ul>
<li><em>Live ping tables</em> are the final destination for the <a href="https://mozilla.github.io/gcp-ingestion/">telemetry ingestion pipeline</a>. Dataflow jobs process incoming ping payloads from clients, batch them together by document type, and load the results to these tables approximately every five minutes, although a few document types are opted in to a more expensive streaming path that makes records available in BigQuery within seconds of ingestion. These tables are partitioned by date according to <code>submission_timestamp</code> and are also clustered on that same field, so it is possible to make efficient queries over short windows of recent data such as the last hour. They have a rolling expiration period of 30 days, but that window may be shortened in the future. Analyses should only use these tables if they need results for the current (partial) day.</li>
<li><em>Historical ping tables</em> have exactly the same schema as their corresponding live ping tables, but they are populated only once per day via an Airflow job and have a 25 month retention period. These tables are superior to the live ping tables for historical analysis because they never contain partial days, they have additional deduplication applied, and they are clustered on <code>sample_id</code>, allowing efficient queries on a 1% sample of clients. It is guaranteed that <code>document_id</code> is distinct within each UTC day of each historical ping table, but it is still possible for a document to appear multiple times if a client sends the same payload across multiple days. Note that this requirement is relaxed for older telemetry ping data that was backfilled from AWS; approximately 0.5% of documents are duplicated in <code>telemetry.main</code> and other historical ping tables for 2019-04-30 and earlier dates.</li>
<li><em>Derived tables</em> are populated by nightly <a href="https://workflow.telemetry.mozilla.org/home">Airflow</a> jobs and are considered an implementation detail; their structure may change at any time at the discretion of the data platform team to allow refactoring or efficiency improvements.</li>
<li><em>User-facing views</em> are the schema objects that users are primarily expected to use in analyses. Many of these views correspond directly to an underlying historical ping table or derived table, but they provide the flexibility to hide deprecated columns or present additional calculated columns to users. These views are the schema contract with users and they should not change in backwards-incompatible ways without a version increase or an announcement to users about a breaking change.</li>
</ul>
<p>Spark and other applications relying on the BigQuery Storage API for data access need to reference derived tables or historical ping tables directly rather than user-facing views. Unless the query result is relatively large, we  recommend instead that users run a query on top of user-facing views with the output saved in a destination table, which can then be accessed from Spark.</p>
<h3><a class="header" href="#structure-of-ping-tables-in-bigquery" id="structure-of-ping-tables-in-bigquery">Structure of Ping Tables in BigQuery</a></h3>
<p>Unlike with the previous AWS-based data infrastructure, we don't have different mechanisms for accessing entire pings vs. &quot;summary&quot; tables. As such, there are no longer special libraries or infrastructure necessary for accessing full pings, rather each document type maps to a user-facing view that can be queried in STMO. For example:</p>
<ul>
<li>&quot;main&quot; pings are accessible from view <code>telemetry.main</code></li>
<li>&quot;crash&quot; pings are accessible from view <code>telemetry.crash</code></li>
<li>&quot;baseline&quot; pings for Fenix are accessible from view <code>org_mozilla_fenix.baseline</code></li>
</ul>
<p>All fields in the incoming pings are accessible in these views, and (where possible) match the nested data structures of the original JSON. Field names are converted from <code>camelCase</code> form to <code>snake_case</code> for consistency and SQL compatibility.</p>
<p>Any fields not present in the ping schemas are present in an <code>additional_properties</code> field containing leftover JSON. BigQuery provides <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions">functions for parsing and manipulating JSON data via SQL</a>.</p>
<p>Later in this document, we demonstrate the use of a few Mozilla-specific
functions that we have defined to allow ergonomic querying of
<a href="#accessing-map-like-fields">map-like fields</a> (which are represented as arrays of structs in BigQuery) and
<a href="#accessing-histograms">histograms</a> (which are encoded as raw JSON strings).</p>
<h3><a class="header" href="#writing-queries" id="writing-queries">Writing Queries</a></h3>
<p>To query a BigQuery table you will need to specify the dataset and table name. It is good practice to specify the project however depending on which project the query
originates from this is optional.</p>
<pre><code class="language-sql">SELECT
    col1,
    col2
FROM
    `project.dataset.table`
WHERE
    -- data_partition_field will vary based on table
    date_partition_field &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 1 MONTH)
</code></pre>
<p>An example query from <a href="../datasets/bigquery/clients_last_seen/reference.html">Clients Last Seen Reference</a></p>
<pre><code class="language-sql">SELECT
    submission_date,
    os,
    COUNT(*) AS count
FROM
    telemetry.clients_last_seen
WHERE
    submission_date &gt;= DATE_SUB(CURRENT_DATE, INTERVAL 1 WEEK)
    AND days_since_seen = 0
GROUP BY
    submission_date,
    os
HAVING
    count &gt; 10 -- remove outliers
    AND lower(os) NOT LIKE '%windows%'
ORDER BY
    os,
    submission_date DESC
</code></pre>
<p>Check out the <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators">BigQuery Standard SQL Functions &amp; Operators</a> for detailed documentation.</p>
<h3><a class="header" href="#writing-query-results-to-a-permanent-table" id="writing-query-results-to-a-permanent-table">Writing query results to a permanent table</a></h3>
<p>You can write query results to a BigQuery table you have access via <a href="bigquery.html#gcp-bigquery-console">GCP BigQuery Console</a> or <a href="bigquery.html#gcp-bigquery-api-access">GCP BigQuery API Access</a></p>
<ul>
<li>Use <code>moz-fx-data-shared-prod.analysis</code> dataset.
<ul>
<li>Prefix your table with your username. If your username is <code>username@mozilla.com</code> create a table with <code>username_my_table</code>.</li>
</ul>
</li>
<li>See <a href="https://cloud.google.com/bigquery/docs/writing-results">Writing query results</a> documentation for detailed steps.</li>
</ul>
<h3><a class="header" href="#writing-results-to-gcs-object-store" id="writing-results-to-gcs-object-store">Writing results to GCS (object store)</a></h3>
<p>If a BigQuery table is not a suitable destination for your analysis results,
we also have a GCS bucket available for storing analysis results. It is usually
Spark jobs that will need to do this.</p>
<ul>
<li>Use bucket <code>gs://moz-fx-data-prod-analysis/</code>
<ul>
<li>Prefix object paths with your username. If your username is <code>username@mozilla.com</code>, you might store a file to <code>gs://moz-fx-data-prod-analysis/username/myresults.json</code>.</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#creating-a-view" id="creating-a-view">Creating a View</a></h3>
<p>You can create views in BigQuery if you have access via <a href="bigquery.html#gcp-bigquery-console">GCP BigQuery Console</a> or <a href="bigquery.html#gcp-bigquery-api-access">GCP BigQuery API Access</a>.</p>
<ul>
<li>Use <code>moz-fx-data-shared-prod.analysis</code> dataset.
<ul>
<li>Prefix your view with your username. If your username is <code>username@mozilla.com</code> create a table with <code>username_my_view</code>.</li>
</ul>
</li>
<li>See <a href="https://cloud.google.com/bigquery/docs/views">Creating Views</a> documentation for detailed steps.</li>
</ul>
<h3><a class="header" href="#using-udfs" id="using-udfs">Using UDFs</a></h3>
<p>BigQuery offers <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions">user-defined functions</a> (UDFs) that can be defined in SQL or JavaScript as part of a query or as a persistent function stored in a dataset. We have defined a suite of persistent functions to enable transformations specific to our data formats, available in datasets <code>udf</code> (for functions defined in SQL) and <code>udf_js</code> (for functions defined in JavaScript). Note that JavaScript functions are potentially much slower than those defined in SQL, so use functions in <code>udf_js</code> with some caution, likely only after performing aggregation in your query.</p>
<p>We document a few of the most broadly useful UDFs below, but you can see the full list of UDFs with source code in <a href="https://github.com/mozilla/bigquery-etl/tree/master/udf"><code>bigquery-etl/udf</code></a> and <a href="https://github.com/mozilla/bigquery-etl/tree/master/udf_js"><code>bigquery-etl/udf_js</code></a>. Publishing a full reference page for our persistent UDFs is a planned improvement, tracked in <a href="https://github.com/mozilla/bigquery-etl/issues/228"><code>bigquery-etl#228</code></a>.</p>
<h4><a class="header" href="#accessing-map-like-fields" id="accessing-map-like-fields">Accessing map-like fields</a></h4>
<p>BigQuery currently lacks native map support and our workaround is to use a STRUCT type with fields named [key, value]. We've created a UDF that provides key-based access with the signature: <code>udf.get_key(&lt;struct&gt;, &lt;key&gt;)</code>. The example below generates a count per <code>reason</code> key in the <code>event_map_values</code> field in the telemetry events table for Normandy unenrollment events from yesterday.</p>
<pre><code class="language-sql">SELECT udf.get_key(event_map_values, 'reason') AS reason,
       COUNT(*) AS EVENTS
FROM telemetry.events
WHERE submission_date = DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)
  AND event_category='normandy'
  AND event_method='unenroll'
GROUP BY 1
ORDER BY 2 DESC
</code></pre>
<h4><a class="header" href="#accessing-histograms" id="accessing-histograms">Accessing histograms</a></h4>
<p>We considered many potential ways to represent histograms as BigQuery fields
and found the most efficient encoding was actually to leave them as raw JSON
strings. To make these strings easier to use for analysis, you can convert them
into nested structures using <code>udf.json_extract_histogram</code>:</p>
<pre><code class="language-sql">WITH
  extracted AS (
  SELECT
    submission_timestamp,
    udf.json_extract_histogram(payload.histograms.a11y_consumers) AS a11y_consumers
  FROM
    telemetry.main )
  --
SELECT
  a11y_consumers.bucket_count,
  a11y_consumers.sum,
  a11y_consumers.range[ORDINAL(1)] AS range_low,
  udf.get_key(a11y_consumers.values, 11) AS value_11
FROM
  extracted
WHERE
  a11y_consumers.bucket_count IS NOT NULL
  AND DATE(submission_timestamp) = &quot;2019-08-09&quot;
LIMIT
  10
</code></pre>
<h1><a class="header" href="#query-optimizations" id="query-optimizations">Query Optimizations</a></h1>
<p>To improve query performance and minimize the cost associated with using BigQuery please see the following query optimizations:</p>
<ul>
<li>Avoid <code>SELECT *</code> by selecting only the columns you need
<ul>
<li>Using <code>SELECT *</code> is the most expensive way to query data. When you use <code>SELECT *</code> <em>BigQuery does a full scan of every column in the table.</em></li>
<li>Applying a <code>LIMIT</code> clause to a <code>SELECT *</code> query might not affect the amount of data read, depending on the table structure.
<ul>
<li>Many of our tables are configured to use <em>clustering</em> in which case a <code>LIMIT</code> clause does effectively limit the amount of data that needs to be scanned.</li>
<li>Tables that include a <code>sample_id</code> field will usually have that as one of the clustering fields and you can efficiently scan random samples of users by specifying <code>WHERE sample_id = 0</code> (1% sample), <code>WHERE sample_id &lt; 10</code> (10% sample), etc. This can be especially helpful with <code>main_summary</code>, <code>clients_daily</code>, and <code>clients_last_seen</code> which are very large tables and are all clustered on <code>sample_id</code>.</li>
<li>To check whether your <code>LIMIT</code> and <code>WHERE</code> clauses are actually improving performance, you should see a lower value reported for actual &quot;Data Scanned&quot; by a query compared to the prediction (&quot;This query will process X bytes&quot;) in STMO or the BigQuery UI.</li>
</ul>
</li>
<li>If you are experimenting with data or exploring data, use one of the <a href="https://cloud.google.com/bigquery/docs/best-practices-costs#preview-data">data preview options</a> instead of <code>SELECT *</code>.
<ul>
<li>Preview support is coming soon to BigQuery data sources in <a href="https://sql.telemetry.mozilla.org/">re:dash</a></li>
</ul>
</li>
</ul>
</li>
<li>Limit the amount of data scanned by using a date partition filter
<ul>
<li>Tables that are larger than 1 TB will require that you provide a date partition filter as part of the query.</li>
<li>You will receive an error if you attempt to query a table that requires a partition filter.
<ul>
<li><code>Cannot query over table 'moz-fx-data-shared-prod.telemetry_derived.main_summary_v4' without a filter over column(s) 'submission_date' that can be used for partition elimination</code></li>
</ul>
</li>
<li>See <a href="bigquery.html#writing-queries"><em>Writing Queries</em></a> for examples.</li>
</ul>
</li>
<li>Reduce data before using a JOIN
<ul>
<li>Trim the data as early in the query as possible, before the query performs a JOIN. If you reduce data early in the processing cycle, shuffling and other complex operations only execute on the data that you need.</li>
<li>Use sub queries with filters or intermediate tables or views as a way of decreasing sides of a join, prior to the join itself.</li>
</ul>
</li>
<li>Do not treat WITH clauses as prepared statements
<ul>
<li>WITH clauses are used primarily for readability because they are not materialized. For example, placing all your queries in WITH clauses and then running UNION ALL is a misuse of the WITH clause. If a query appears in more than one WITH clause, it executes in each clause.</li>
</ul>
</li>
<li>Use approximate aggregation functions
<ul>
<li>If the SQL aggregation function you're using has an equivalent approximation function, the approximation function will yield faster query performance. For example, instead of using <code>COUNT(DISTINCT)</code>, use <code>APPROX_COUNT_DISTINCT()</code>.</li>
<li>See <a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#approximate-aggregate-functions">approximate aggregation functions</a> in the standard SQL reference.</li>
</ul>
</li>
<li>Reference the data size prediction (&quot;This query will process X bytes&quot;) in STMO and the BigQuery UI to help gauge the efficiency of your queries. You should see this number go down as you limit the range of <code>submission_date</code>s or include fewer fields in your <code>SELECT</code> statement. For clustered tables, this estimate won't take into account benefits from <code>LIMIT</code>s and <code>WHERE</code> clauses on clustering fields, so you'll need to compare to the actual &quot;Data Scanned&quot; after the query is run. <a href="https://cloud.google.com/bigquery/pricing#on_demand_pricing">Queries are charged by data scanned at $5/TB</a> so each 200 GB of data scanned will cost $1; it can be useful to keep the data estimate below 200 GB while developing and testing a query to limit cost and query time, then open up to the full range of data you need when you have confidence in the results.</li>
</ul>
<p>A complete list of optimizations can be found <a href="https://cloud.google.com/bigquery/docs/best-practices-performance-overview">here</a> and cost optimizations <a href="https://cloud.google.com/bigquery/docs/best-practices-costs">here</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                            <a rel="prev" href="../cookbooks/index.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        

                        
                            <a rel="next" href="../cookbooks/bigquery-airflow.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                    <a href="../cookbooks/index.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a href="../cookbooks/bigquery-airflow.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="../elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="../mermaid.min.js"></script>
        
        <script type="text/javascript" src="../mermaid-init.js"></script>
        

        

    </body>
</html>
